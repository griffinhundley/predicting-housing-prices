{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Housing Prices in King County, Washington"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Griffin Hundley, Malcolm Katzenbach, and Lauren Phipps\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview  <a id=\"Overview\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A one-paragraph overview of the project, including the business problem, data, methods, results and recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1. Business Problem](#Business-Problem)\n",
    "\n",
    "[2. Data Understanding](#Data-Understanding)\n",
    "\n",
    "[3. Data Cleaning](#Data-Cleaning)\n",
    "\n",
    "[4. Data Modeling](#Data-Modeling)\n",
    "\n",
    "[5. Evaluation](#Evaluation)\n",
    "\n",
    "[6. Conclusion](#Conclusion)\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Problem  <a id=\"Business-Problem\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When listing a house for sale, it is important to price it accurately. Houses that are priced too high can sit on the market for months, which can lead to the price eventually being dropped to generate sales. According to a study from Zillow, in the Seattle area, a house that is on the market for 2 months will lead to a 5% drop in price. On the other side, if a house is priced too low, a seller is not maximizing the return on their house. In both of these cases, the seller is not getting as much money as they can and the realtor is not maximizing their comission for the sale, as commission is tied to selling price. \n",
    "\n",
    "To combat this, we are creating a model to estimate the price of a home based on certain characterisitics of the house. This will allow realtors to more accurately price the house to ensure it sells quickly and at a fair price. \n",
    "\n",
    "The questions we are targeting are:\n",
    "\n",
    "   1) Which house features most impact the price of house?\n",
    "\n",
    "   2) How do these feature impact the price of a house?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Summary of the business problem you are trying to solve, and the data questions that you plan to answer to solve them.\n",
    "\n",
    "Questions to consider:\n",
    "\n",
    "What are the business's pain points related to this project?\n",
    "How did you pick the data analysis question(s) that you did?\n",
    "Why are these questions important from a business perspective?\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding  <a id=\"Data-Understanding\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data being used is publicly available housing data from King County, Washington from 2014-2015. The dataset contains ~21,600 rows, with each row representing the sale of a house in King County. The columns represent features of that house. The target variable is the price of the house, which will be used to determine the effect of several features on houses.  Prior to any data analysis, the test data is split from the training data to prevent data leakage. The split is 80% training, 20% testing.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Index(['id', 'date', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot',\n",
    "       'floors', 'waterfront', 'grade', 'sqft_above', 'yr_built',\n",
    "       'sqft_living15', 'sqft_lot15', 'has_been_renovated', 'ratio_15',\n",
    "       'years_old', 'condition_2', 'condition_3', 'condition_4',\n",
    "       'condition_5'],\n",
    "      dtype='object')```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions to consider:\n",
    "\n",
    "Where did the data come from, and how do they relate to the data analysis questions?\n",
    "What do the data represent? Who is in the sample and what variables are included?\n",
    "What is the target variable?\n",
    "What are the properties of the variables you intend to use?\n",
    "\n",
    "Import packages, run code, explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datacleaning'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9040c4f526c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# from src import datacleaning as dc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# from src import linregvis as lrv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdatacleaning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlinregvis\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlrv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datacleaning'"
     ]
    }
   ],
   "source": [
    "# import sys\n",
    "# import os\n",
    "# module_path = os.path.abspath(os.path.join(os.pardir, os.pardir))\n",
    "# if module_path not in sys.path:\n",
    "#     sys.path.append(module_path)\n",
    "# from src import datacleaning as dc\n",
    "# from src import linregvis as lrv\n",
    "import datacleaning as dc\n",
    "import linregvis as lrv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as stats\n",
    "from statsmodels.formula.api import ols\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.dummy import DummyRegressor\n",
    "from statsmodels.graphics.regressionplots import abline_plot\n",
    "from sklearn.metrics import mean_squared_error\n",
    "%matplotlib inline\n",
    "sns.set_style('darkgrid')\n",
    "pd.options.display.float_format = \"{:,.2f}\".format\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.ticker as mtick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the initial data and splitting into test and training sets in an 80:20 split\n",
    "df = dc.test_split(\n",
    "    dc.get_data('data/kc_house_data.csv')\n",
    ")\n",
    "X_train = df[0]\n",
    "X_test = df[1]\n",
    "y_train = df[2]\n",
    "y_test = df[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation  <a id=\"Data-Preparation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step was to split the data into the training set and the test set. This was done as an 80%/20% split, respectively.\n",
    "\n",
    "From here we dropped the following columns: view, lat, long, zipcode, and sqft_basement. We changed the date to a datetime datatype and filled in the null values for waterfront as zeroes since most homes are not waterfront and it would likely be something that was specified if so. We also created two new features. First is ratio15, which is the ratio of the size of the house compared to the homes around it, using the sqft_living and sqft_living15. The other is years old, with the year built subtracted from 2020. Lastly, we created dummy variables for the conditions. \n",
    "\n",
    "***\n",
    "Questions to consider:\n",
    "\n",
    "Were there variables you dropped or created?\n",
    "How did you address missing values or outliers?\n",
    "Why are these choices appropriate given the data and the business problem?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning, dropping unused columns, feature engineering, dummy columns\n",
    "X_train = dc.clean_it(df[0])\n",
    "X_test = dc.clean_it(df[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Modeling  <a id=\"Data-Modeling\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in modeling was to look at the relationship between price (the target variable) and each of the features in the data set. This was done by creating a pairplot. These pairplots show that there is a somewhat linear relationship between price and bathrooms, living space (sqft_living), and grade. Describe and justify the process for analyzing or modeling the data.\n",
    "***\n",
    "Questions to consider:\n",
    "\n",
    "How did you analyze or model the data?\n",
    "How did you iterate on your initial approach to make it better?\n",
    "Why are these choices appropriate given the data and the business problem?\n",
    "\n",
    "Run code to model data\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessment of Linearity Between Price and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break the columns in groups to plot 4 on a row at a time\n",
    "df = pd.concat([X_train, y_train], axis = 1)\n",
    "n = 4\n",
    "features = list(X_train.drop(columns = 'date').columns)\n",
    "row_groups= [features[i:i+n] for i in range(0, len(features), n) ]\n",
    "for i in row_groups:\n",
    "    pp = sns.pairplot(data=df, y_vars=['price'],x_vars=i, kind=\"reg\", height=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relationship between living space and price looks the strongest so that was the feature we started to investigate further. Looking at just that variable we made a regression plot that also shows the distribution of each variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([X_train, y_train], axis = 1)\n",
    "fig = sns.jointplot(x=\"sqft_living\", y=\"price\", data=df, kind=\"reg\", height = 8, ratio = 5)\n",
    "fig.fig.suptitle(\"Relationship Between Living Space (ft2) and House Price\", fontsize = 20)\n",
    "fig.fig.subplots_adjust(top=0.95)\n",
    "fig.set_axis_labels(xlabel = 'Living Space, (ft2)', ylabel = 'Price', fontsize = 14)\n",
    "sns.set(font_scale = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a linear relationship between the variables and both distributions have a slight skew. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Assessment of Correlation Between Features\n",
    "\n",
    "Finally, we created a heat map to investigate the linearity within the features to address any possible issues of colinearity in our model as we add in additional features. Any features with a correlation value of greater than 0.7 will not be used together in iterations of a model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize = (10, 10))\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "sns.heatmap(df.corr(), cmap = cmap)\n",
    "ax.set_title('Heat Map of Correlation Between Predictors', fontsize = 20)\n",
    "fig.savefig('./images/heatmap.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Model Iteration\n",
    "\n",
    "#### Creation of Dummy Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we created a dummy regressor model as a baseline. The dummy regressor uses the default of predicting the mean of the training set and was fitted to the our training X and y. We also checked the baseline score of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = DummyRegressor()\n",
    "dummy.fit(X_train, y_train)\n",
    "dummy.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find our baseline error, we predicted the y values with our model for the X_train values. My using the mean_squared_error from sklearn metrics, we found by Root Mean Square Error between our predicted and known y_train values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dummy.predict(X_train)\n",
    "dummy_rmse = np.sqrt(mean_squared_error(y_train, y_pred))\n",
    "dummy_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first model we checked was by setting only variable to sq_ft_living. This variable was chosen due to how most people commonly think of the square footage of the house. By using the Ordinary Least Squares function we fitted, we created our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = \"price ~ sqft_living\"\n",
    "model_1 = lrv.run_lr(formula, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the summary of our model we first check the R-squared value, which tells us the percentage of the variance in price can be explained by this model. It can only explain 48.9% of the variance in price. Then by checking the p value for the coefficient of the sqft_living variable to check if it is signficant to our model or not. Using a confidence level of 0.05, because the p value is less than 0025, we can reject the null hypothesis that the coefficient is not significantly different from zero. The coefficient for sqft_living tells us that for every square foot in the house, the price of the house increase by $281.61. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create the y prediction by using the X_train values using this first model. The Root Mean Squared Error is then calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_1.predict(X_train)\n",
    "model_1_rmse = np.sqrt(mean_squared_error(y_train, y_pred))\n",
    "model_1_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Root Mean Squared Error (RMSE) is equal to 265871.88, which is less than the RMSE for the baseline model. This indicates that this model is better than our baseline model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the first model being only a basic linear regression model, we graph a scatter plot of the price vs. sqft_living and draw the regression line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (14,8))\n",
    "ax = sns.scatterplot(X_train.sqft_living, y_train)\n",
    "ax.set_xlabel('Square Foot Living Space (ft2)', fontsize = 14)\n",
    "ax.set_ylabel('Price', fontsize = 14)\n",
    "ax.set_title('Effect of living space on Housing Price', fontsize = 24)\n",
    "ax.tick_params(labelsize = 14)\n",
    "lrv.dollar_ticks(ax)\n",
    "abline_plot(model_results=model_1, ax = ax);\n",
    "fig.savefig('./images/simple_linear_regression.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2\n",
    "\n",
    "The second model expands on the previous model by checking the condition of the house. Using the condition variable we created dummy variables for each level of condition of which there are 5. Using the ols function and fitting the data, we created the second model with price the dependent variable and sqft_living and our condition dummy variables as the independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = \"price ~ sqft_living + condition_2 + condition_3 + condition_4 + condition_5\"\n",
    "model_2 = lrv.run_lr(formula, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In model 2, the R-squared value is 0.496. This is only .008 better than the previous model. When the p values are examined, the sqft_living is still equal to 0.000, however the conditions all have a p value greater than 0.1. Keeping are confidence levels the same, we cannot reject the null hypothesis that the coefficient is significantly different from zero. So we can conclude that condition feature is not significantly important to the price value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_2.predict(X_train)\n",
    "model_2_rmse = np.sqrt(mean_squared_error(y_train, y_pred))\n",
    "model_2_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the RMSE of model two confirms what are R-squared value implied. The RMSE decreased 1000 from the previous model, which is not significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 3\n",
    "\n",
    "In the third model, the condition dummy variables are removed from the formula due to not being significant and expanded by adding grade, waterfront, years_old, and ratio_15. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = \"price ~ sqft_living + grade + waterfront + years_old + ratio_15\"\n",
    "model_3 = lrv.run_lr(formula, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In model 3, the R-squared value is 0.636. This is an increase of 0.14 or 14% than the previous model. The p values for all the features are 0.000, thus for all coefficients the null hypothesis that the coefficient is not significantly different from zero is rejected. All the new features are significant in the prediction of price and increases the percentage of variance in price that the model is able to explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_3.predict(X_train)\n",
    "model_3_rmse = np.sqrt(mean_squared_error(y_train, y_pred))\n",
    "model_3_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMSE from model 3 is 224526, which is a decrease of nearly 40000. In addition to the R-squared value, this model is much better at predicting the price of homes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 4\n",
    "\n",
    "In the fourth model, the sqft_living is removed due the high mult-linear correlation between the variable and grade and ratio_15. Bathrooms has a high multi-linear correlation with sqft_living, but not as high as with grade or ratio_15. We use bathrooms as a possible replacement for sqft_living."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = \"price ~ bathrooms + grade + waterfront + years_old + ratio_15\"\n",
    "model_4 = lrv.run_lr(formula, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fourth model has an R-squared value of 0.594. This is less than the 0.636 of the previous model. Though it decreases possible error due to multi-linear correlation. The p values for the features used are all 0.000, thus rejecting the null hypothesis that the coefficient is not significantly different from zero. Taking into account the multi-linear correlation, this model is the best improvement from the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_4.predict(X_train)\n",
    "model_4_rmse = np.sqrt(mean_squared_error(y_train, y_pred))\n",
    "model_4_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMSE is equal 236951 which is an increase from model three, but still a 30000 decrease from model 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the fifth model, the feature has_been_renovated was added to see if renovating a house has a predictable impact on the price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "formula = \"price ~ bathrooms + grade + waterfront + years_old + ratio_15 + has_been_renovated\"\n",
    "model_5 = lrv.run_lr(formula, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fifth model gives an identical $R^2$ value, and the p-value is greater than the significance level $\\alpha=0.05$ so this is not a statistically significant relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_5.predict(X_train)\n",
    "model_5_rmse = np.sqrt(mean_squared_error(y_train, y_pred))\n",
    "model_5_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMSE is barely a decrease at all from the previous model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sixth model, because has_been_renovated and ratio_15 had little impact on the effect, they are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "formula = \"price ~ bathrooms + grade + waterfront + years_old\"\n",
    "model_6 = lrv.run_lr(formula, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_6.predict(X_train)\n",
    "model_6_rmse = np.sqrt(mean_squared_error(y_train, y_pred))\n",
    "model_6_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = model_6\n",
    "test = X_test.sample(1, random_state = 500)\n",
    "coef = final_model.params\n",
    "est_price = coef[0] + coef[1]* test.bathrooms + coef[2]* test.grade + coef[3]* test.waterfront + coef[4]*test.years_old\n",
    "print(f'The estimate price is ${int(est_price)}')\n",
    "print(f'The real price is ${y_test[test.index[0]]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation of Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis of Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmean = final_model.resid.mean()\n",
    "mmin = final_model.resid.min()\n",
    "mmax = final_model.resid.max()\n",
    "normalized_residuals = (final_model.resid - mmean)/(mmax - mmin)\n",
    "\n",
    "t = final_model.fittedvalues\n",
    "tmean = final_model.resid.mean()\n",
    "tmin = final_model.resid.min()\n",
    "tmax = final_model.resid.max()\n",
    "normalized_predicted = (t - tmean)/(tmax - tmin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The assumption of normality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (14,8))\n",
    "ax.set_xlabel('Normalized Residuals', fontsize = 14)\n",
    "ax.set_ylabel('Frequency', fontsize = 14)\n",
    "ax.set_title('Frequency of Normalized Residuals', fontsize = 24)\n",
    "ax.set(xlim=(-0.25,.25))\n",
    "sns.distplot(normalized_residuals, bins = 100);\n",
    "#fig.savefig('./images/residuals_hist.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the figure above the residuals of the model are roughly normally distributed, so it passes the assumption of normality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Homoskedasticity Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (14,8))\n",
    "ax.set_xlabel('Normalized Predicted Price', fontsize = 14)\n",
    "ax.set_ylabel('Normalized Residuals', fontsize = 14)\n",
    "ax.set_title('Residuals vs Predicted Values for Homoskedasticity', fontsize = 24)\n",
    "sns.scatterplot(x = normalized_predicted, y = normalized_residuals);\n",
    "#fig.savefig('./images/homoskedasticity.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the homoskedasticity test, there is an apparent bias.  This implies that there are underlying variables that influence the price that are not captured in the features of this particular model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QQ Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (12, 8))\n",
    "stats.probplot(normalized_residuals, plot=plt)\n",
    "ax.set_title('QQ Plot Probability of residuals', fontsize = 24)\n",
    "ax.set_xlabel('Residuals', fontsize = 14)\n",
    "ax.set_ylabel('Probability', fontsize = 14)\n",
    "ax.legend(['Residuals','Normal line']);\n",
    "#fig.savefig('./images/qqplot.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability plot shows the probability of the residuals of being normally distributed.  The tails of the residuals tend away from the normality line, implying that the residuals become less normal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation  <a id=\"Evaluation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = final_model.predict(X_test)\n",
    "final_model_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "final_model_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this particular set of training data, the model was able to predict the test data slightly better than the training data, which implies that this model is generalizable to new incoming data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate how well your work solves the stated business problem.\n",
    "\n",
    "Questions to consider:\n",
    "\n",
    "How do you interpret the results?\n",
    "How well does your model fit your data? How much better is this than your baseline model?\n",
    "How confident are you that your results would generalize beyond the data you have?\n",
    "How confident are you that this model would benefit the business if put into use?\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion  <a id=\"Conclusion\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide your conclusions about the work you've done, including any limitations or next steps.\n",
    "\n",
    "Questions to consider:\n",
    "\n",
    "What would you recommend the business do as a result of this work?\n",
    "What are some reasons why your analysis might not fully solve the business problem?\n",
    "What else could you do in the future to improve this project?\n",
    "\n",
    "***\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
